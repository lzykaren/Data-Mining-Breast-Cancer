{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8d7fdae8-1d31-4873-a021-d553e2c4087c",
    "_execution_state": "idle",
    "_uuid": "c28fa7775a99901a882aee31e890ea99fe796d91"
   },
   "source": [
    "# Data Mining Analysis on Breast Cancer Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "52942f7b-e58d-4275-86f0-ced1bcea06f9",
    "_execution_state": "idle",
    "_uuid": "d7dc365d2933b6675c57c98d438356e4cc1e6125",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization library  \n",
    "import matplotlib.pyplot as plt\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import time\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c9bd4680-5a5d-4ce5-8b85-1820d2e478d0",
    "_execution_state": "idle",
    "_uuid": "4a65810840012b075b5a359994931bec8acf9ab0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d30f1486-bb97-40d7-9125-67e6f15286dc",
    "_execution_state": "idle",
    "_uuid": "3e01972c830afa1ce55025c0b7a202d4b204dd1d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()  # head method show only first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60308baf-344a-41fb-8580-cef707ce5aa8",
    "_execution_state": "idle",
    "_uuid": "54600377cdbec016505dcb970bb1988afbc260a2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature names as a list\n",
    "col = data.columns       # .columns gives columns names in data \n",
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8764b4cf-9963-4c1a-b449-059de8153e4c",
    "_execution_state": "idle",
    "_uuid": "94ea75618315ac7af54cf80a501c42b40e77ecbc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y includes our labels and x includes our features\n",
    "y = data.diagnosis                          # M or B \n",
    "list = ['Unnamed: 32','id','diagnosis']\n",
    "x = data.drop(list,axis = 1 )   # drop useless features\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "31ec8d06-ea25-4b34-84ca-c322b3d8a10f",
    "_execution_state": "idle",
    "_uuid": "71fecf26e957a2d670182d607aca5a7b92b4a3b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = sns.countplot(y,label=\"Count\")# M = 212, B = 357\n",
    "ax.grid(False)\n",
    "ax.set_xticklabels([\"Malignant\",\"Benign\"])\n",
    "B, M= y.value_counts()\n",
    "print('Number of Benign: ',B)\n",
    "print('Number of Malignant : ',M)\n",
    "print(\"M/B percentage: \",M/B)  # to check the possible selection bias for future model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c92292c2-d999-42f3-8618-73b231c163e6",
    "_execution_state": "idle",
    "_uuid": "7d909ed445dd83306413a72986cebc17d1814cc7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6179a010-0819-481e-8095-72ba1021fdcd",
    "_execution_state": "idle",
    "_uuid": "3edac4b24f82f00d32efe9d812aed40fb06fdbed"
   },
   "source": [
    "# Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d58052d6-9e8c-46f4-a2d5-b9e82b247f27",
    "_execution_state": "idle",
    "_uuid": "d640909614b5ff561e35b33e555458df70b22486",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first ten features\n",
    "data_dia = y\n",
    "data = x\n",
    "data_n_2 = (data - data.mean()) / (data.std())              # standardization\n",
    "data = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "46ee71d3-93c1-4c2d-bb00-6995f7a1c816",
    "_execution_state": "idle",
    "_uuid": "0a18301387ce26b58a68e5a2d340b39e86c1f5e0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second ten features\n",
    "data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "58f17ef1-6530-4db8-bcd9-32691363e8a9",
    "_execution_state": "idle",
    "_uuid": "d1c4e84c3d6bda4b9ff9e284d8c790dd46980c31",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Third ten features\n",
    "data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54c8d1570b21a65ea707c21440d90f6701b15e2b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use boxplot to check  similarity\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "47880bbb-5dbe-4836-938c-0816a03e8fb4",
    "_execution_state": "idle",
    "_uuid": "859ec665af4be178c3e36b1c2799f44c5ccef901",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x.loc[:,'concavity_worst'], x.loc[:,'concave points_worst'], kind=\"regg\", color=\"#ce1414\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3bda33fe-daf9-4f74-acbc-d9d3c8fc83d9",
    "_execution_state": "idle",
    "_uuid": "381ecb55ced22383c96320ced2299f5da37ce4b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "df = x.loc[:,['radius_worst','perimeter_worst','area_worst']]\n",
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "g.map_upper(plt.scatter)\n",
    "g.map_diag(sns.kdeplot, lw=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ef378d49-8aed-4b9e-96e8-e7d2458fdd89",
    "_execution_state": "idle",
    "_uuid": "85a2413b70c1b3d69f26a2c122c22d55f930e774",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "data_dia = y\n",
    "data = x\n",
    "data_n_2 = (data - data.mean()) / (data.std())              # standardization\n",
    "data = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "tic = time.time()\n",
    "sns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n",
    "\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "428c75b6-b5d0-47e3-a568-17b5d1896c0c",
    "_execution_state": "idle",
    "_uuid": "75dfd5e9e50adceb1d42dd000ce779e79b069cce",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee64bbc8-0431-482a-b08f-cdca43e41390",
    "_execution_state": "idle",
    "_uuid": "209e9e9120d6e889696d2d1190e663b5c1885a82",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n",
    "toc = time.time()\n",
    "plt.xticks(rotation=90)\n",
    "print(\"swarm plot time: \", toc-tic ,\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e1e7d8a-bbf2-4aab-90e7-78d4c4ccf416",
    "_execution_state": "idle",
    "_uuid": "0eeb70ddffc8ac332ee076f2f6b2833a6ffddd2d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#correlation map\n",
    "f,ax = plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6786734a-40a9-46b6-a13a-97ee9c569636",
    "_execution_state": "idle",
    "_uuid": "84b145dd0c13a3a0d4dd4f9b9fd1bd782e11fcf8"
   },
   "source": [
    "# Feature Selection and Random Forest Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c7b2df4e-270e-4c94-8789-177f5e90ac46",
    "_execution_state": "idle",
    "_uuid": "a042df90ef7138d6f101463e93936119176bdc0d"
   },
   "source": [
    "In this part we will select feature with different methods that are feature selection with correlation, univariate feature selection, recursive feature elimination (RFE), recursive feature elimination with cross validation (RFECV) and tree based feature selection. We will use random forest classification in order to train our model and predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "94d217e3-b2b3-4016-b72e-f8d521d17af7",
    "_execution_state": "idle",
    "_uuid": "39003c7b75f265bf0826f407433e65923c4dd017"
   },
   "source": [
    "## 1) Feature selection with correlation and random forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ef8d06df-bfcc-4e9a-a3ba-5016ec0c5bd5",
    "_execution_state": "idle",
    "_uuid": "117f3e858e806f3f26a68dadf3fc89d471010156",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\n",
    "x_1 = x.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later \n",
    "x_1.head()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "733f0784-4a3f-410c-a220-f98591825f2e",
    "_execution_state": "idle",
    "_uuid": "eec5424039036e1af43ba0795b76393805308f97",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#correlation map\n",
    "f,ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "111af932-96f8-4105-8deb-ba1172edd203",
    "_execution_state": "idle",
    "_uuid": "c7a6af60a44959f81593d788934a49c9259d8b43",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# split data train 80 % and test 20 %\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#random forest classifier with n_estimators=10 (default)\n",
    "clf_rf = RandomForestClassifier(random_state=43)      \n",
    "clr_rf = clf_rf.fit(x_train,y_train)\n",
    "\n",
    "ac = accuracy_score(y_test,clf_rf.predict(x_test))\n",
    "print('Accuracy is: ',ac)\n",
    "cm = confusion_matrix(y_test,clf_rf.predict(x_test))\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3eed9ac3-e601-4e16-85bc-26a1a6fff850",
    "_execution_state": "idle",
    "_uuid": "decd86422aee506b061c905e8573abb3612734e4"
   },
   "source": [
    "## 2) Univariate feature selection and random forest classification\n",
    "In univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features.\n",
    "<http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest>\n",
    "we consider choose top 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4f43c8bd-48f7-4ed9-aa2d-6aa8a29c0c58",
    "_execution_state": "idle",
    "_uuid": "8159f9efb106f1219dc4e8c2a340399b88f224d8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "# find best scored 5 features\n",
    "select_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\n",
    "feature_importance={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c9684618-06fe-4b0a-835f-ceea46da397c",
    "_execution_state": "idle",
    "_uuid": "d9dcd1495cbf33c190a0d1211df4bac5e79bc4e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Score list:', select_feature.scores_)\n",
    "print('Feature list:', x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "efc70e04-bc9c-4f93-bcd3-b1d7160d0d5c",
    "_execution_state": "idle",
    "_uuid": "9a2bd21537f7c600f3c9baaf833c001084d6ba00",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_2 = select_feature.transform(x_train)\n",
    "x_test_2 = select_feature.transform(x_test)\n",
    "#random forest classifier with n_estimators=10 (default)\n",
    "clf_rf_2 = RandomForestClassifier()      \n",
    "clr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\n",
    "ac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\n",
    "print('Accuracy is: ',ac_2)\n",
    "cm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\n",
    "sns.heatmap(cm_2,annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c20adf88b966e3cc56f05163c19a4fd2907c6dc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression(random_state=0, solver='lbfgs',class_weight=True)\n",
    "scores = cross_val_score(clf, x_train_2,y_train, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6cacbf4c04f981274fa1b49cc06ed0ce3d16b68c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_scores(model,x_train_2,y_train):\n",
    "    clf=model\n",
    "    scores = cross_val_score(clf, x_train_2,y_train, cv=10)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3744c29cfe9d5bf96a1d69d107259ad62444c65d"
   },
   "source": [
    "### Here we try several classification algorithm like Logistic Regression, Naive Bayes, SVM Linear, QDA, KNN to see different model's performance on difference feature space(5 features and 15 features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c47a1f3a1c9dc6f54f2750969faec58094921c2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_list=[LogisticRegression(random_state=0, solver='lbfgs',class_weight=True),GaussianNB(priors=[0.6,0.4]),SVC(class_weight=\"balanced\",kernel=\"linear\"),QuadraticDiscriminantAnalysis(priors=[0.6,0.4]),KNeighborsClassifier(n_neighbors=3)]\n",
    "model_name=[\"Logistic Regression\",\"Naive Bayes\",\"SVM Linear\",\"QDA\",\"KNN\"]\n",
    "performance_of_5_features_of_train={}\n",
    "print(\"Main 5 features scores with different models in train\")\n",
    "for i,name in enumerate(model_name):\n",
    "    performance_of_5_features_of_train[name]=model_scores(model_list[i],x_train_2,y_train)\n",
    "print(performance_of_5_features_of_train)\n",
    "print(\"All features scores with different models in train\")    \n",
    "performance_of_15_features_of_train={}\n",
    "for i,name in enumerate(model_name):\n",
    "    performance_of_15_features_of_train[name]=model_scores(model_list[i],x_train,y_train)\n",
    "print(performance_of_15_features_of_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88ba3b3c88587b7a769500c6b4e251c011ed8e6b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "fig=figure(figsize=(20,10))\n",
    "ax1=subplot(121)\n",
    "ax1=scatter(performance_of_5_features_of_train.keys(),performance_of_5_features_of_train.values())\n",
    "ylim(0.8,1.0)\n",
    "ax2=subplot(122)\n",
    "ax2=scatter(performance_of_15_features_of_train.keys(),performance_of_15_features_of_train.values())\n",
    "ylim(0.8,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73f1158a883f90b99b363bbecb0d9ec9c1244c1b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Main 5 features scores with different models in test\")\n",
    "performance_of_5_features_in_test={}\n",
    "performance_of_15_features_in_test={}\n",
    "for i,name in enumerate(model_name):\n",
    "    performance_of_5_features_in_test[name]=model_scores(model_list[i],x_test_2,y_test)\n",
    "print(performance_of_5_features_in_test)\n",
    "print(\"All features scores with different models in test\")    \n",
    "for i,name in enumerate(model_name):\n",
    "    performance_of_15_features_in_test[name]=model_scores(model_list[i],x_test,y_test)\n",
    "print(performance_of_15_features_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27447c343c199a61b37813cfe9b3e3bc2002037b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig=figure(figsize=(20,10))\n",
    "ax1=subplot(121)\n",
    "ax1=scatter(performance_of_5_features_in_test.keys(),performance_of_5_features_in_test.values())\n",
    "ylim(0.8,1.0)\n",
    "ax2=subplot(122)\n",
    "ax2=scatter(performance_of_15_features_in_test.keys(),performance_of_15_features_in_test.values())\n",
    "ylim(0.8,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7a1734077908a92776eb67299b1a05803dc0f1b"
   },
   "source": [
    "###  We try to see our model's performances on the train and test based on different feature space. The following is 15 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a49cc695588d6f23e5c45ff7bbc2b7f3a0a4ea8b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig=figure(figsize=(20,10))\n",
    "grid(False)\n",
    "scatter(performance_of_15_features_of_train.keys(),performance_of_15_features_of_train.values(),color=\"r\",s=100)\n",
    "scatter(performance_of_15_features_in_test.keys(),performance_of_15_features_in_test.values(),color=\"b\",s=100)\n",
    "ylim(0.8,1.0) \n",
    "legend([\"train\",\"test\"],fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3787b4a3cfc52b816c3487f2781158c85d4619b9"
   },
   "source": [
    "### Here we try to see whether our 5 features perform well on the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "663d7569419a270682a75d1d5d92dd064a0c2ff1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig=figure(figsize=(20,10))\n",
    "scatter(performance_of_5_features_of_train.keys(),performance_of_5_features_of_train.values(),c=\"r\")\n",
    "scatter(performance_of_5_features_of_train.keys(),performance_of_5_features_in_test.values(),c=\"b\")\n",
    "ylim(0.8,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8b89151710b7feccf236a139a551e69043fb03c"
   },
   "source": [
    "### Our findings is interesting:  apparently some classification algorithm has overfitting during crossvalidation when we use all features to predict. However, when we try to use just five main features model to test our algorithm performance, almost all models have better performances. This implies if we use all features, we have overfitting problem. At the same time, the main five features could better indicate or explain whether someone would have cancer.   What we can learn from these is another interesting phenomenon since QDA has better performance if we use all features instead of 5 features whenever we are training or testing.  However, QDA has weaker interpretability as logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "42b533046cf91df2e038f9f8163df8cd1c23aea4"
   },
   "source": [
    "## Our conclusion: 1. only 5 main features really matter.  2. Logistic regression is a pretty good model to do classification in this case in terms of performance and interpretability. \n",
    "                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "702ad2b3-5b12-4d15-93b1-e7d62dfd1040",
    "_execution_state": "idle",
    "_uuid": "7a3c3050dd9d694e52962c7c712b1ea16aab6fdf"
   },
   "source": [
    "## 3) Recursive feature elimination (RFE) with random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8df88bb5-8003-4696-9efe-63ebf8d609a5",
    "_execution_state": "idle",
    "_uuid": "c384a5240d1c1e9e2a6750e5d218dadaf24d2035",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "# Create the RFE object and rank each pixel\n",
    "clf_rf_3 = RandomForestClassifier()      \n",
    "rfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\n",
    "rfe = rfe.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "51d63d0b-4e00-4dc1-816c-809287b60806",
    "_execution_state": "idle",
    "_uuid": "29ba35a98954d0ae686ce46295179d1f1a27b74c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92aa6013-3e16-4005-ab1b-b7ce53e78bd3",
    "_execution_state": "idle",
    "_uuid": "ce670f778a661e8ddc3b7b21a43ccb48a551581a"
   },
   "source": [
    "Chosen 5 best features by rfe is **texture_mean, area_mean, concavity_mean, area_se, concavity_worst**. They are exactly similar with previous (selectkBest) method. Therefore we do not need to calculate accuracy again. Shortly, we can say that we make good feature selection with rfe and selectkBest methods. However as you can see there is a problem, okey I except we find best 5 feature with two different method and these features are same but why it is **5**. Maybe if we use best 2 or best 15 feature we will have better accuracy. Therefore lets see how many feature we need to use with rfecv method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "22a4f840-2a37-4047-9804-129e7f68f74a",
    "_execution_state": "idle",
    "_uuid": "42a8c3f2ef0e5978b620eea737e6e234dc79cfe8"
   },
   "source": [
    "## 4) Recursive feature elimination with cross validation and random forest classification\n",
    "<http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html>\n",
    "Now we will not only **find best features** but we also find **how many features do we need** for best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7a5d4d69-7734-4465-89cc-f46b4af4c548",
    "_execution_state": "idle",
    "_uuid": "0d7803966979745a8bdbdbc44a1927558485640a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "clf_rf_4 = RandomForestClassifier() \n",
    "rfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\n",
    "rfecv = rfecv.fit(x_train, y_train)\n",
    "\n",
    "print('Optimal number of features :', rfecv.n_features_)\n",
    "print('Best features :', x_train.columns[rfecv.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b69144b-72e4-4ac3-b8a8-c9ebbf8ffa3b",
    "_execution_state": "idle",
    "_uuid": "f362bfa341032f2bb1bacc1c50675a1916f5c536",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot number of features VS. cross-validation scores\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score of number of selected features\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2637e8bc-d986-41c0-acef-ce76afc4c350",
    "_execution_state": "idle",
    "_uuid": "8bc3105398fc618e19deec4de957950cfb45c054"
   },
   "source": [
    "## 5) Tree based feature selection and random forest classification\n",
    "<http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>\n",
    "In random forest classification method there is a **feature_importances_** attributes that is the feature importances (the higher, the more important the feature). **!!! To use feature_importance method, in training data there should not be correlated features. Random forest choose randomly at each iteration, therefore sequence of feature importance list can change.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df8abc8d-3279-4c31-a6b6-e4f272ca0b47",
    "_execution_state": "idle",
    "_uuid": "31d4b248f723930ff7120ffaff2c260f07e3f0fc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf_5 = RandomForestClassifier()      \n",
    "clr_rf_5 = clf_rf_5.fit(x_train,y_train)\n",
    "importances = clr_rf_5.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "\n",
    "plt.figure(1, figsize=(14, 13))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(x_train.shape[1]), importances[indices],\n",
    "       color=\"g\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\n",
    "plt.xlim([-1, x_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "00008678-9012-4e50-aaab-010b3353ac98",
    "_execution_state": "idle",
    "_uuid": "760b045b33388f6fb7b53acdf931e8204eea80cd"
   },
   "source": [
    "As you can seen in plot above, after 5 best features importance of features decrease. Therefore we can focus these 5 features. As I sad before, I give importance to understand features and find best of them. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
